{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8a38a621",
      "metadata": {
        "id": "8a38a621"
      },
      "source": [
        "# Automated DPO Pipeline with Django Integration\n",
        "This notebook automates DPO (Direct Preference Optimization) training using feedback from your Django backend, including score and API response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "05363e28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "05363e28",
        "outputId": "34217208-7418-4b22-c01c-17e54295ea11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 — Install required libraries\n",
        "!pip install -U transformers datasets trl accelerate peft bitsandbytes requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "D0s1ihyy6MLq",
      "metadata": {
        "id": "D0s1ihyy6MLq"
      },
      "outputs": [],
      "source": [
        "# Cell 2 — Setup API\n",
        "USERNAME = \"emotuna_user\"\n",
        "API_BASE = \"https://73099cf3fccd.ngrok-free.app\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "cbd6ef83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbd6ef83",
        "outputId": "e0f30683-1f0e-4d31-cbe3-2962dd904e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded SFT dataset, status: 200\n"
          ]
        }
      ],
      "source": [
        "# Cell 3 — Download SFT dataset\n",
        "import requests\n",
        "\n",
        "DATASET_URL = f'{API_BASE}/api/dataset/?username={USERNAME}'\n",
        "response = requests.get(DATASET_URL)\n",
        "with open('sft_dataset.jsonl', 'wb') as f:\n",
        "    f.write(response.content)\n",
        "print('Downloaded SFT dataset, status:', response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ef128bb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef128bb5",
        "outputId": "f28158b0-0485-41a6-873f-9362f3de7182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 13 examples\n",
            "{'user_message': 'You are a complete mess', 'api_response': 'I’m sorry you feel that way—let me know what I can fix.', 'score': '60', 'edited_reply': 'Noted with thanks.'}\n",
            "                     user_message  \\\n",
            "0         You are a complete mess   \n",
            "1  Have you taken your breakfast?   \n",
            "2           Wanna grab lunch tgt?   \n",
            "3           wanna grab lunch tgt?   \n",
            "4           wanna grab lunch tgt?   \n",
            "\n",
            "                                        api_response score  \\\n",
            "0  I’m sorry you feel that way—let me know what I...    60   \n",
            "1   Yep, just finished—fuelled up and ready to roll!    70   \n",
            "2            Yes! I’m starving—where should we meet?    30   \n",
            "3                  Already ate, but down for coffee!   100   \n",
            "4  I'm not sure if you're aware of this, but I've...     0   \n",
            "\n",
            "                               edited_reply  \n",
            "0                        Noted with thanks.  \n",
            "1  Yes, I have just completed my breakfast.  \n",
            "2                            No. I am busy.  \n",
            "3         Already ate, but down for coffee!  \n",
            "4                       i am busy right now  \n"
          ]
        }
      ],
      "source": [
        "# Cell 4 — Load & preview dataset\n",
        "import json, os, zipfile, shutil\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "with open('sft_dataset.jsonl', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "print('Loaded', len(data), 'examples')\n",
        "print(data[0])  # Show the first example\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "_nVPG4-Aq1AM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nVPG4-Aq1AM",
        "outputId": "4f08a139-8dcb-42f2-8b16-0edbe2754f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dpo_dataset.jsonl already exists\n"
          ]
        }
      ],
      "source": [
        "# Cell 5 — Copy dataset for DPO if not exists\n",
        "if not os.path.exists('dpo_dataset.jsonl'):\n",
        "    shutil.copy('sft_dataset.jsonl', 'dpo_dataset.jsonl')\n",
        "    print('Copied sft_dataset.jsonl to dpo_dataset.jsonl')\n",
        "else:\n",
        "    print('dpo_dataset.jsonl already exists')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "de4e6248",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "de4e6248",
        "outputId": "99de2198-d76c-4e2a-a614-c3dc73e95c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/dpo_trainer.py:151: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  prompt_input_ids = [torch.tensor(example[\"prompt_input_ids\"]) for example in examples]\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/dpo_trainer.py:153: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  chosen_input_ids = [torch.tensor(example[\"chosen_input_ids\"]) for example in examples]\n",
            "/usr/local/lib/python3.12/dist-packages/trl/trainer/dpo_trainer.py:155: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  rejected_input_ids = [torch.tensor(example[\"rejected_input_ids\"]) for example in examples]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 00:04, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.593400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DPO training complete!\n"
          ]
        }
      ],
      "source": [
        "# Cell 6 — Train DPO (with updated dataset format)\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "\n",
        "# ------------------------------\n",
        "# 0. Disable WandB\n",
        "# ------------------------------\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Load model & tokenizer\n",
        "# ------------------------------\n",
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure pad token exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Load & preprocess dataset\n",
        "# ------------------------------\n",
        "dataset = load_dataset(\"json\", data_files=\"dpo_dataset.jsonl\", split=\"train\")\n",
        "\n",
        "def preprocess_dpo(example):\n",
        "    return {\n",
        "        \"query\": example[\"user_message\"],\n",
        "        \"chosen\": example[\"edited_reply\"],   # always preferred\n",
        "        \"rejected\": example[\"api_response\"]  # less preferred\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(preprocess_dpo)\n",
        "\n",
        "# Ensure dataset outputs torch.long tensors\n",
        "dataset.set_format(type=\"torch\")\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Define DPOConfig\n",
        "# ------------------------------\n",
        "training_args = DPOConfig(\n",
        "    output_dir=\"./gpt2-dpo\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",   # disables wandb/tensorboard\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Initialize DPOTrainer\n",
        "# ------------------------------\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer   # ✅ correct for your TRL version\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# 5. Train\n",
        "# ------------------------------\n",
        "trainer.train()\n",
        "print(\"✅ DPO training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explicitly save final model\n",
        "SAVE_DIR = \"./gpt2-dpo-final\"\n",
        "trainer.model.save_pretrained(SAVE_DIR)\n",
        "trainer.tokenizer.save_pretrained(SAVE_DIR)\n",
        "print(f\"✅ Model saved to {SAVE_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMSRZmtQJeky",
        "outputId": "14bc5c10-c9b6-43de-b733-e4aab947776a"
      },
      "id": "WMSRZmtQJeky",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved to ./gpt2-dpo-final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "1f3ea508",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f3ea508",
        "outputId": "20269561-cc85-4de7-9651-0aaf32b2624a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Files in archive:\n",
            "['vocab.json', 'model.safetensors', 'config.json', 'merges.txt', 'generation_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json']\n"
          ]
        }
      ],
      "source": [
        "import shutil, os\n",
        "\n",
        "REQUIRED_FILES = [\n",
        "    'pytorch_model.bin', 'model.safetensors', 'config.json', 'tokenizer.json',\n",
        "    'vocab.json', 'merges.txt', 'tokenizer_config.json',\n",
        "    'special_tokens_map.json', 'generation_config.json'\n",
        "]\n",
        "\n",
        "SAVE_DIR = \"./gpt2-dpo-final\"\n",
        "\n",
        "# Remove any junk files\n",
        "for file in os.listdir(SAVE_DIR):\n",
        "    if file not in REQUIRED_FILES:\n",
        "        os.remove(os.path.join(SAVE_DIR, file))\n",
        "\n",
        "# Zip the directory\n",
        "shutil.make_archive(\"dpo_model\", \"zip\", SAVE_DIR)\n",
        "\n",
        "# Verify contents\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"dpo_model.zip\", \"r\") as zf:\n",
        "    print(\"📦 Files in archive:\")\n",
        "    print(zf.namelist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "_sm7Mie7COXU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sm7Mie7COXU",
        "outputId": "5b36694f-d9b8-4199-aa9a-e7a6b69b674d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file size: 442.41 MB\n"
          ]
        }
      ],
      "source": [
        "# Cell 9 — Check zip size\n",
        "zip_path = 'dpo_model.zip'\n",
        "size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "print(f\"ZIP file size: {size_mb:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "OAeLYcRf_hnq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAeLYcRf_hnq",
        "outputId": "146cc541-faa1-4510-fe19-3ce1244243d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 5 chunks\n",
            "Uploading dpo_model.zip.part0 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '73099cf3fccd.ngrok-free.app'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload status code for dpo_model.zip.part0: 204\n",
            "dpo_model.zip.part0 uploaded successfully\n",
            "Uploading dpo_model.zip.part1 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '73099cf3fccd.ngrok-free.app'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload status code for dpo_model.zip.part1: 204\n",
            "dpo_model.zip.part1 uploaded successfully\n",
            "Uploading dpo_model.zip.part2 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '73099cf3fccd.ngrok-free.app'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload status code for dpo_model.zip.part2: 204\n",
            "dpo_model.zip.part2 uploaded successfully\n",
            "Uploading dpo_model.zip.part3 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '73099cf3fccd.ngrok-free.app'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload status code for dpo_model.zip.part3: 204\n",
            "dpo_model.zip.part3 uploaded successfully\n",
            "Uploading dpo_model.zip.part4 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '73099cf3fccd.ngrok-free.app'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload status code for dpo_model.zip.part4: 204\n",
            "dpo_model.zip.part4 uploaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Cell 10 — Split into chunks & upload\n",
        "MODEL_UPLOAD_URL = f'{API_BASE}/api/model/'\n",
        "zip_file_path = 'dpo_model.zip'\n",
        "chunk_size = 100 * 1024 * 1024  # 100 MB\n",
        "\n",
        "def split_file(file_path, chunk_size=chunk_size):\n",
        "    chunks = []\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        chunk_num = 0\n",
        "        while True:\n",
        "            chunk = f.read(chunk_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "            chunk_name = f\"{file_path}.part{chunk_num}\"\n",
        "            with open(chunk_name, \"wb\") as chunk_file:\n",
        "                chunk_file.write(chunk)\n",
        "            chunks.append(chunk_name)\n",
        "            chunk_num += 1\n",
        "    return chunks\n",
        "\n",
        "chunk_files = split_file(zip_file_path)\n",
        "print(f\"Created {len(chunk_files)} chunks\")\n",
        "\n",
        "for chunk_file in chunk_files:\n",
        "    print(f\"Uploading {chunk_file} ...\")\n",
        "    with open(chunk_file, 'rb') as f:\n",
        "        files = {'file': f}\n",
        "        data = {'username': USERNAME}\n",
        "        try:\n",
        "            r = requests.post(MODEL_UPLOAD_URL, files=files, data=data, verify=False)\n",
        "            print(f\"Upload status code for {chunk_file}: {r.status_code}\")\n",
        "            if r.status_code in [200, 201, 204]:\n",
        "                print(f\"{chunk_file} uploaded successfully\")\n",
        "            else:\n",
        "                print(f\"Failed to upload {chunk_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error uploading {chunk_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a7e37025",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7e37025",
        "outputId": "dff7dd08-7430-4157-cc3f-12506642afc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '73099cf3fccd.ngrok-free.app'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model unzipped successfully on server.\n"
          ]
        }
      ],
      "source": [
        "# Cell 11 — Trigger unzip on server\n",
        "MODEL_UNZIP_URL = f'{API_BASE}/api/model/unzip/'\n",
        "try:\n",
        "    r = requests.post(MODEL_UNZIP_URL, data={'username': USERNAME}, verify=False)\n",
        "    if r.status_code == 200:\n",
        "        print('✅ Model unzipped successfully on server.')\n",
        "    else:\n",
        "        print(f'⚠️ Failed to unzip model on server. Status code: {r.status_code}, Response: {r.text}')\n",
        "except Exception as e:\n",
        "    print(f'Error sending unzip request: {e}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}